{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env settings:\n",
      "Keras==2.1.5\n",
      "tensorflow==1.6.0\n"
     ]
    }
   ],
   "source": [
    "!echo 'Current env settings:'\n",
    "!pip freeze | grep Keras\n",
    "!pip freeze | grep sorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzfelix/miniconda3/envs/example/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Dense, Input,LSTM, TimeDistributed, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from layers.crf import ChainCRF\n",
    "from utils import loader\n",
    "from utils import sequential\n",
    "from utils import featurizer\n",
    "\n",
    "EMBEDDING_PATH = 'levy_deps.words'\n",
    "WV_DIMENSIONS = 300\n",
    "\n",
    "MAX_SEQUENCE_LEN = 10\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT_PROB = 0.25\n",
    "LSTM_OUTPUT_SIZE = 100\n",
    "\n",
    "MODEL_DUMP = 'trained_model.h5'\n",
    "\n",
    "DATASET_TRAIN = 'dataset/train.txt'\n",
    "DATASETS_DEV  = 'dataset/dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(model_data):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    history = model_data.history\n",
    "\n",
    "    plt.subplot(121)\n",
    "    ran = np.arange(len(history['val_loss']))\n",
    "\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.plot(ran, history['loss'], '-or', alpha=0.5)\n",
    "    plt.plot(ran, history['val_loss'], '-ob', alpha=0.5)\n",
    "    plt.legend(['Train', 'Dev'], loc='best')\n",
    "    plt.title('Loss');\n",
    "\n",
    "    plt.subplot(122)\n",
    "    ran = np.arange(len(history['sparse_categorical_accuracy']))\n",
    "\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.plot(ran, history['sparse_categorical_accuracy'], '-or', alpha=0.5)\n",
    "    plt.plot(ran, history['val_sparse_categorical_accuracy'], '-ob', alpha=0.5)\n",
    "    plt.legend(['Train', 'Dev'], loc='best')\n",
    "    plt.title('Accuracy');\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo sample:\n",
      "\n",
      "Text:   ['I', 'will', 'never', 'return', 'there', 'again', '(', 'and', 'now', 'have', 'some', 'serious', 'doubts', 'about', 'the', 'quality', 'of', 'work', 'they', 'actually', 'performed', 'on', 'my', 'car', ')', '.']\n",
      "Labels: ['PRON', 'AUX', 'ADV', 'VERB', 'ADV', 'ADV', 'PUNCT', 'CONJ', 'ADV', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'PRON', 'ADV', 'VERB', 'ADP', 'PRON', 'NOUN', 'PUNCT', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = loader.load_dataset(DATASET_TRAIN)\n",
    "x_dev,   y_dev   = loader.load_dataset(DATASETS_DEV)\n",
    "\n",
    "vocab = loader.build_vocabulary(x_train)\n",
    "wv, word2index = loader.load_word_embeddings(EMBEDDING_PATH,\n",
    "                                             WV_DIMENSIONS)\n",
    "\n",
    "print('Demo sample:\\n')\n",
    "print('Text:   {}'.format(x_train[-1]))\n",
    "print('Labels: {}'.format(y_train[-1]))\n",
    "\n",
    "assert len(x_train) == len(y_train)\n",
    "assert len(x_dev) == len(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics:\n",
      "\t# words:   174020\n",
      "\t# classes: 17\n",
      "\t# train samples: 12543\n",
      "\t# dev   samples: 2002\n",
      "\n",
      "Demo sample: \n",
      "\tText:   ['I', 'will', 'never', 'return', 'there', 'again', '(', 'and', 'now', 'have']\n",
      "\tTokens: [ 29 198 948 419  81 537  12   5 313  15]\n",
      "\tLabels: [10  3  2 15  2  2 12  4  2 15]\n"
     ]
    }
   ],
   "source": [
    "pos_encoder = sequential.SequenceEncoder()\n",
    "case_encoder = sequential.SequenceEncoder()\n",
    "\n",
    "x_train_emb = featurizer.map_words(x_train, word2index)\n",
    "x_dev_emb   = featurizer.map_words(x_dev, word2index)\n",
    "\n",
    "y_train_emb = pos_encoder.fit_transform(y_train)\n",
    "y_dev_emb   = pos_encoder.transform(y_dev)\n",
    "\n",
    "# padding && picking just part of the data\n",
    "# pad sequences and discard words from the RIGHT\n",
    "pad = lambda x: pad_sequences(x, MAX_SEQUENCE_LEN,\n",
    "                              truncating='post', padding='post')\n",
    "\n",
    "x_train_emb = pad(x_train_emb)\n",
    "y_train_emb = pad(y_train_emb)\n",
    "\n",
    "x_dev_emb = pad(x_dev_emb)\n",
    "y_dev_emb = pad(y_dev_emb)\n",
    "\n",
    "# making output episodic\n",
    "y_train_emb = np.expand_dims(y_train_emb, -1)\n",
    "y_dev_emb = np.expand_dims(y_dev_emb, -1)\n",
    "\n",
    "print('Statistics:')\n",
    "print('\\t# words:   {}'.format(len(word2index)))\n",
    "print('\\t# classes: {}'.format(len(pos_encoder.classes_)))\n",
    "print('\\t# train samples: {}'.format(len(x_train)))\n",
    "print('\\t# dev   samples: {}'.format(len(x_dev)))\n",
    "\n",
    "print('\\nDemo sample: ')\n",
    "print('\\tText:   {}'.format(x_train[-1][:MAX_SEQUENCE_LEN]))\n",
    "print('\\tTokens: {}'.format(x_train_emb[-1]))\n",
    "print('\\tLabels: {}'.format(np.ravel(y_train_emb[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(wv, n_classes):\n",
    "    \"\"\"\n",
    "    Builds POS prediction model:\n",
    "    Embedding -> projection -> CRF\n",
    "    \"\"\"\n",
    "    vocab_size, embedding_dim = wv.shape\n",
    "\n",
    "    sentence_in = Input(shape=(None,), name='input_sentence')\n",
    "    word_embedding = Embedding(vocab_size, embedding_dim, weights=[wv],\n",
    "                       trainable=False, name='embedding')(sentence_in)\n",
    "    \n",
    "    projection = Dense(n_classes)(word_embedding)\n",
    "    \n",
    "    crf = ChainCRF(name='chain_crf')\n",
    "    output = crf(projection)\n",
    "    loss_fun = crf.sparse_loss\n",
    "    \n",
    "    model = Model(inputs=[sentence_in], outputs=[output])\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile('nadam', loss_fun,\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_sentence (InputLayer)  (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 300)         52206000  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 17)          5117      \n",
      "_________________________________________________________________\n",
      "chain_crf (ChainCRF)         (None, None, 17)          323       \n",
      "=================================================================\n",
      "Total params: 52,211,440\n",
      "Trainable params: 5,440\n",
      "Non-trainable params: 52,206,000\n",
      "_________________________________________________________________\n",
      "Train on 12543 samples, validate on 2002 samples\n",
      "Epoch 1/5\n",
      "12543/12543 [==============================] - 15s 1ms/step - loss: 14.0002 - sparse_categorical_accuracy: 0.8014 - val_loss: 15.5350 - val_sparse_categorical_accuracy: 0.8950\n",
      "Epoch 2/5\n",
      " 7488/12543 [================>.............] - ETA: 5s - loss: 5.4597 - sparse_categorical_accuracy: 0.8767"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "\n",
    "n_classes = len(pos_encoder.classes_)\n",
    "model = build_model(wv, n_classes)\n",
    "\n",
    "metrics = model.fit(x_train_emb, y_train_emb, BATCH_SIZE, epochs=5,\n",
    "          validation_data=(x_dev_emb, y_dev_emb));\n",
    "model.save_weights(MODEL_DUMP)\n",
    "\n",
    "plot_curves(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dumped model and comparing the results\n",
    "\n",
    "pred_old = model.predict(x_dev_emb)\n",
    "\n",
    "new_model = build_model(wv, n_classes)\n",
    "new_model.load_weights(MODEL_DUMP)\n",
    "pred_new = new_model.predict(x_dev_emb)\n",
    "\n",
    "assert np.array_equal(pred_old, pred_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (example)",
   "language": "python",
   "name": "example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
